{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bixi Telemetry Data Warehouse\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The goal of this project is to build a data warehouse for Bixi, a Montreal-based bike-sharing service. The modeled data will then be used to analyze ride pattern and availability of bikes at every station, enabling Bixi to optimize replenishment activities.\n",
    "\n",
    "The project follows the following steps:,\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import psycopg2\n",
    "import sql_queries\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBFS_SOURCE = \"s3a://bixi-gbfs-data\"\n",
    "GBFS_DESTINATION = \"s3a://bixi-gbfs-data-parquet\"\n",
    "\n",
    "STATION_INFO_DATA = \"https://gbfs.velobixi.com/gbfs/en/station_information.json\"\n",
    "TRIP_APRIL_DATA = \"https://sitewebbixi.s3.amazonaws.com/uploads/docs/biximontreal-rentals-2021-04-87aaed.zip\"\n",
    "TRIP_APRIL_DATA_FILE = \"OD_2021_04.csv\""
   ]
  },
  {
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "This project will outline the steps to obtain the raw data, transform and clean it, write it back to an S3 bucket and finally injest it into a Redshift datawarehouse. The source data will be collected from GBFS feed data from Bixi that had been previously extracted every 5 minutes for several weeks and stored in an S3 bucket. For. this project, 14 days worth of GBFS feed was gathered for a total of 2100 JSON files of 693 records each, totalling roughly 1.4M records. The latest station metadata and historical trip data will be retrieved from the Bixi REST API directly. The data will be read, cleansed and transfered using a Spark cluster, for efficiency given the large data size. The data warehouse will be hosted on an Amazon Redshift cluster.\n",
    "\n",
    "#### Data \n",
    "The main dataset used will be a json output of the Bixi API, which exposes their [GBFS (General Bikeshare Feed Specification)](https://github.com/NABSA/gbfs) feed. The API was queries every 5 minutes for 3 weeks, using a Python script scheduled by a cron job. The response json was then uploaded to an Amazon S3 bucket.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Setting up the Spark cluster and reading in the data\n",
    "\n",
    "We start by provisioning the AWS access ID and secret key in the spark config and instantiate a spark session"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.expanduser(\"~/.aws/credentials\"))\n",
    "access_id = config.get(\"default\", \"aws_access_key_id\") \n",
    "access_key = config.get(\"default\", \"aws_secret_access_key\")\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT = config.get(\"DWH\",\"DWH_PORT\")\n",
    "DWH_ENDPOINT = config.get(\"DWH\",\"DWH_ENDPOINT\")\n",
    "DWH_IAM_ROLE_NAME = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.0')\n",
    "conf.set(\"fs.s3a.access.key\", access_id)\n",
    "conf.set(\"fs.s3a.secret.key\", access_key)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "source": [
    "Once the Spark session is created, all the `json` files in the S3 bucket are then read into a Spark dataframe. The nested station data is exploded to individual rows and split by column. The unix epoch timestamp in column `last_updated` is converted to a timestamp, then several time parts are expanded into multiple column, which will later be used to partition the parquet output "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(f\"{GBFS_SOURCE}/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = (df\n",
    "    .select(\"last_updated\",explode(\"data.stations\").alias('stations'))\n",
    "    .select(\"last_updated\", \"stations.*\")    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = df_flat \\\n",
    "    .withColumn(\"last_updated_dt\", F.from_unixtime(\"last_updated\").cast('timestamp')) \\\n",
    "    .withColumn(\"station_id\", df_flat.station_id.cast('long')) \\\n",
    "    .withColumn('is_installed', df_flat.is_installed.cast('boolean')) \\\n",
    "    .withColumn('is_renting', df_flat.is_renting.cast('boolean')) \\\n",
    "    .withColumn('is_returning', df_flat.is_returning.cast('boolean')) \\\n",
    "    .withColumn(\"year\", F.year(\"last_updated_dt\")) \\\n",
    "    .withColumn(\"month\", F.month(\"last_updated_dt\")) \\\n",
    "    .withColumn(\"day\", F.dayofmonth(\"last_updated_dt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = df_flat.select(\n",
    "    [\"station_id\"] + [c for c in df_flat.columns if c not in [\n",
    "        'station_id','eightd_has_available_keys','last_updated'\n",
    "    ]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- station_id: long (nullable = true)\n |-- is_charging: boolean (nullable = true)\n |-- is_installed: boolean (nullable = true)\n |-- is_renting: boolean (nullable = true)\n |-- is_returning: boolean (nullable = true)\n |-- last_reported: long (nullable = true)\n |-- num_bikes_available: long (nullable = true)\n |-- num_bikes_disabled: long (nullable = true)\n |-- num_docks_available: long (nullable = true)\n |-- num_docks_disabled: long (nullable = true)\n |-- num_ebikes_available: long (nullable = true)\n |-- last_updated_dt: timestamp (nullable = true)\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- day: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_flat.printSchema()"
   ]
  },
  {
   "source": [
    "Next, the Bixi API is called again to obtain the latest station metadata"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output = \"data/station_info.json\"\n",
    "with open(json_output, 'w') as f:\n",
    "    json.dump(requests.get(STATION_INFO_DATA).json(),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_station = spark.read.json(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_station_flat = df_station \\\n",
    "    .select(explode(\"data.stations\").alias('stations')) \\\n",
    "    .select(\"stations.station_id\"\n",
    "        , \"stations.name\"\n",
    "        , 'stations.lat'\n",
    "        , \"stations.lon\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- station_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- lat: double (nullable = true)\n |-- lon: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_station_flat.printSchema()"
   ]
  },
  {
   "source": [
    "The last dataset is trip data from the month april 2021, which is the same timeframe as the GBFS feed, which needs to be extracted from a `zip` file, read into a spark dataframe from a `csv` file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'data/OD_2021_04.csv'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "csv_path = f\"data/{TRIP_APRIL_DATA_FILE}\"\n",
    "\n",
    "r = requests.get(TRIP_APRIL_DATA)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extract(TRIP_APRIL_DATA_FILE, path = 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips = spark.read.csv(csv_path, header=True)\n",
    "df_trips = df_trips.withColumn(\"trip_id\",F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips = df_trips \\\n",
    "    .withColumn(\"start_date\", F.from_utc_timestamp(\"start_date\",\"EST\")) \\\n",
    "    .withColumn(\"end_date\", F.from_utc_timestamp(\"end_date\",\"EST\")) \\\n",
    "    .withColumn(\"duration_sec\", df_trips.duration_sec.cast('int')) \\\n",
    "    .withColumn(\"start_station_id\", df_trips.emplacement_pk_start.cast('long')) \\\n",
    "    .withColumn(\"end_station_id\", df_trips.emplacement_pk_end.cast('long')) \\\n",
    "    .withColumn(\"is_member\", df_trips.is_member.cast('boolean')) \\\n",
    "    .withColumn(\"year\", F.year(\"start_date\")) \\\n",
    "    .withColumn(\"month\", F.month(\"start_date\")) \\\n",
    "    .withColumn(\"day\", F.dayofmonth(\"start_date\"))\n",
    "\n",
    "df_trips = df_trips.select([\"trip_id\"] + [c for c in df_trips.columns if c not in [\"trip_id\",\"emplacement_pk_start\",\"emplacement_pk_end\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- trip_id: long (nullable = false)\n |-- start_date: timestamp (nullable = true)\n |-- end_date: timestamp (nullable = true)\n |-- duration_sec: integer (nullable = true)\n |-- is_member: boolean (nullable = true)\n |-- start_station_id: long (nullable = true)\n |-- end_station_id: long (nullable = true)\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- day: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_trips.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "Since this data comes from the Bixi API and comes from automated telemetry, we can assume that the data is faily clean. Neverthless there are likely scenarios we would like to exclude.\n",
    "\n",
    "Let's first look like there are any trip data with 0 duration, missing station, or where the end datetime is smaller or equal to the start datetime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 errors found\n"
     ]
    }
   ],
   "source": [
    "errors = df_trips.where(\n",
    "    df_trips.end_date <= df_trips.start_date\n",
    ").collect()\n",
    "\n",
    "print(f\"{str(len(errors))} errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 errors found\n"
     ]
    }
   ],
   "source": [
    "zero_duration = df_trips.where(\n",
    "    df_trips.duration_sec == 0\n",
    ").collect()\n",
    "\n",
    "print(f\"{str(len(zero_duration))} errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 errors found\n"
     ]
    }
   ],
   "source": [
    "missing_station = df_trips.where(\n",
    "    df_trips.start_station_id.isNull() | \\\n",
    "    df_trips.end_station_id.isNull()\n",
    ").collect()\n",
    "\n",
    "print(f\"{str(len(missing_station))} errors found\")"
   ]
  },
  {
   "source": [
    "No errors found there. As expected, such errors were probably already removed by the Bixi team. We can also check if we have trips that started and ended at the same station. Since this could be a valid use case for someone going on a ride for fun, we can restrict to a duration of no more than 2 min. Such trips are likely not real trips and should be excluded."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "778 errors found\n"
     ]
    }
   ],
   "source": [
    "dummy_trip = df_trips.where(\n",
    "    (df_trips.start_station_id == df_trips.end_station_id) & \\\n",
    "    (df_trips.duration_sec < 120)\n",
    ")\n",
    "\n",
    "print(f\"{str(len(dummy_trip.collect()))} errors found\")"
   ]
  },
  {
   "source": [
    "There are many of such instances. Let us remove them from the data frame"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_clean = df_trips.join(dummy_trip, on='trip_id', how='left_anti')"
   ]
  },
  {
   "source": [
    "Lastly, the transformed and cleaned dataframe are written back to an S3 bucket as `parquet` and `csv` files. The trips and gbfs files are partitioned by year, month, day, given the size and frequency of the raw data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbfs_file_destination = f\"{GBFS_DESTINATION}/gbfs.parquet\"\n",
    "\n",
    "df_flat.write \\\n",
    "    .partitionBy('year','month','day') \\\n",
    "    .parquet(path = gbfs_file_destination, mode = \"overwrite\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_destination = f\"{GBFS_DESTINATION}/stations.csv\"\n",
    "df_station_flat.write.csv(\n",
    "    path = stations_destination, \n",
    "    mode = \"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_destination = f\"{GBFS_DESTINATION}/trips.parquet\"\n",
    "df_trips_clean.write \\\n",
    "    .partitionBy('year','month', 'day') \\\n",
    "    .parquet(\n",
    "        path = trips_destination, \n",
    "        mode = \"overwrite\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "![](resources/erd.png)\n",
    "\n",
    "The stations table is the master dimension table for the bike stations. Both the trips and gbfs tables have foreign key relationships to it. Additionally, a time dimension table was also created containing various time parts of the timestamps found in the fact tables. The goal is to pre-compute these as they will likely be heavily used for analytics purposes. This allows us to improve query performance and reduce cost.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "![](resources/pipeline.png)\n",
    "\n",
    "The first step is a scheduled Python script through cron job that fetches the latest GBFS feed data from Bixi every 5 minutes. The data is saved to a S3 bucket. Once collected, the data is read from S3 by a Spark job on a jupyter notebook. The notebook script also retrives the latest station metadata, along with the last month of trip data. The 3 resulting dataframes are stored to a separate S3 bucket in parquet and csv format. From there, the notebook script executes queries into the Redshift cluster to crate the tables and copy the parquet data to those tables. Finally, the time dimension table is computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "\n",
    "First, we instantiated the connection to the Redshift database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    dbname= DWH_DB, \n",
    "    host= DWH_ENDPOINT, \n",
    "    port= DWH_PORT, \n",
    "    user= DWH_DB_USER, \n",
    "    password= DWH_DB_PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query):\n",
    "    cur = conn.cursor()\n",
    "    results = None\n",
    "    try:\n",
    "        cur.execute(query)\n",
    "        try:\n",
    "            results = cur.fetchall()\n",
    "        except:\n",
    "            pass\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        cur.close()\n",
    "    return results"
   ]
  },
  {
   "source": [
    "Then we create the necessary tables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in sql_queries.create_queries:\n",
    "    run_query(query)"
   ]
  },
  {
   "source": [
    "Finally, we run a query for each table which copies the data from S3 directly into our Redshift tables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_destination = f\"{GBFS_DESTINATION}/trips.parquet\"\n",
    "\n",
    "copy_trips = sql_queries.copy_table_query.format(\n",
    "    'trips',trips_destination.replace('s3a','s3'), access_id, access_key, 'parquet'\n",
    ")\n",
    "run_query(copy_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_destination = f\"{GBFS_DESTINATION}/stations.csv\"\n",
    "copy_stations = sql_queries.copy_table_query.format(\n",
    "    'stations',stations_destination.replace('s3a','s3'), access_id, access_key, 'csv'\n",
    ")\n",
    "run_query(copy_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbfs_file_destination = f\"{GBFS_DESTINATION}/gbfs.parquet\"\n",
    "copy_gbfs= sql_queries.copy_table_query.format(\n",
    "    'gbfs',gbfs_file_destination.replace('s3a','s3'), access_id, access_key, 'parquet'\n",
    ")\n",
    "run_query(copy_gbfs)"
   ]
  },
  {
   "source": [
    "Lastly, the time table is computed from the loaded tables' timestamp columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_query(sql_queries.copy_time_query)"
   ]
  },
  {
   "source": [
    "We can also run a sample query to test the table joins"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(47, 'William / St-Henri', 5, 5050),\n",
       " (147, 'Calixa-LavallÃ©e / Sherbrooke', 6, 2020),\n",
       " (171, 'Wolfe / Robin', 11, 3939),\n",
       " (289, 'Boyer / Beaubien', 14, 4646),\n",
       " (344, 'de la Roche / Everett', 4, 1212)]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "run_query(sql_queries.sample_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "Next, we will ensure the pipeline ran sucessfully by comparing the row count in the source dataframe with the row count of the resulting table. An error will be thrown for any errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_row_count(df_loaded, tbl_name):\n",
    "    df_count = df_loaded.count()\n",
    "    tbl_count = run_query(sql_queries.check_row_count.format(tbl_name))[0][0]\n",
    "    if df_count != tbl_count:\n",
    "        raise ValueError (f\"Data check failed for table {tbl_name}. Row count mistamatch. Expected {df_count}, got {tbl_count}\")\n",
    "    else:\n",
    "        print(f\"Data check passed from table {tbl_name} passed with {tbl_count} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data check passed from table gbfs passed with 1312541 rows\n",
      "Data check passed from table stations passed with 707 rows\n",
      "Data check passed from table trips passed with 237177 rows\n"
     ]
    }
   ],
   "source": [
    "check_row_count(df_flat, 'gbfs')\n",
    "check_row_count(df_station_flat, 'stations')\n",
    "check_row_count(df_trips_clean, 'trips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_primary_key_constraint(tbl_name, col_name):\n",
    "    cnt = run_query(sql_queries.check_primary_key_constraint.format(col_name,tbl_name))[0][0]\n",
    "    if cnt > 0:\n",
    "        raise ValueError (f\"Data check failed for table {tbl_name}. {cnt} values in column {col_name} are not unique\")\n",
    "    else:\n",
    "        print(f\"Data check passed from table {tbl_name}. Primary key constraint respected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data check passed from table trips. Primary key constraint respected.\nData check passed from table stations. Primary key constraint respected.\n"
     ]
    }
   ],
   "source": [
    "check_primary_key_constraint('trips','trip_id')\n",
    "check_primary_key_constraint('stations', 'station_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "`trips`\n",
    "\n",
    "| Column | Data Type | Description |\n",
    "|:--- |:--- |:---|\n",
    "| trip_id | **integer** | Unique identifier for the trips, added during transformation|\n",
    "| start_date | **timestamp** | Start datetime of the trip|\n",
    "| end_date | **timestamp** | End datetime of the trip|\n",
    "| duration_sec | **integer** | Trip duration in seconds|\n",
    "| is_member | **boolean** | Flag identifying if trip was made by a Bixi member|\n",
    "| start_station_id | **integer** | Station identifier where trip began\n",
    "| end_startion_id | **integer** | Station identifier where trip ended\n",
    "\n",
    "`station`\n",
    "\n",
    "| Column | Data Type | Description |\n",
    "|:--- |:--- |:---|\n",
    "| station_id | **integer** | Unique identifier for the station|\n",
    "| name | **varchar** | Long name of the station|\n",
    "| lat | **decimal** | Latitude of the station location|\n",
    "| lon | **decimal** | Longitude of the station location|\n",
    "\n",
    "`gbfs`\n",
    "\n",
    "| Column | Data Type | Description |\n",
    "|:--- |:--- |:---|\n",
    "| station_id | **integer** | Unique identifier for the station|\n",
    "| is_charging | **boolean** | If station is being charged|\n",
    "| is_installed | **boolean** | If station is installed|\n",
    "| is_renting | **boolean** | If station is renting out bikes|\n",
    "| is_returning | **boolean** | If station is raccepting bike returns|\n",
    "| last_reported | **integer** | Epoch unix timestamp when station last reported data|\n",
    "| num_bikes_available | **integer** | Number of bikes available|\n",
    "| num_bikes_disabled | **integer** | Number of bikes disabled|\n",
    "| num_docks_available | **integer** | Number of docks available|\n",
    "| num_docks_disabled| **integer** | ENumber of bikes disabled|\n",
    "| num_ebikes_available | **integer** | Number of electric bikes available|\n",
    "| last_updated_dt | **integer** | Timestamp when GBFS feed API was queried|\n",
    "\n",
    "`time`\n",
    "\n",
    "| Column | Data Type | Description |\n",
    "|:--- |:--- |:---|\n",
    "| datetime | **timestamp** | Unique identifier for the timestamp|\n",
    "| year | **integer** | Year of timestamp|\n",
    "| month | **integer** | Month # of timestamp|\n",
    "| day | **integer** | Day of month of timestamp|\n",
    "| day_of_week | **integer** | Day of week of timestamp|\n",
    "| hour | **integer** | Hour of timestamp|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "Summary of steps taken for this project:\n",
    "1. The first step was to routinely call the Bixi GBFS api to grab the latest station data and push the response content to an S3 bucket\n",
    "2. Next, a Redshift cluster was setup on AWS and provisioned with the appropriate permissions\n",
    "3. Then, a Spark cluster was set up to handle the data load and transformations\n",
    "4. The Jupyter notebook script starts by retrieving the GBFS data from S3 and reads it into a dataframe. The latest station information and historical trip data is also retrieved from the Bixi API and read into separate dataframe\n",
    "5. The script then transforms the data by casting to specific data types, and computing year, month and day columns which are then used to partition the parquet files\n",
    "6. Some assessment is then done on the data. The trip data is cleansed by removing outlier trips, defined as starting and ending at the same station for a duration of less than 2 minutes.\n",
    "7. The dataframes are then written back to a separate S3 bucket, in parquet or csv format, with appropriate partitionning. \n",
    "8. Finally, a connection to the Redshift cluster is instantiated, and a query is run to first create the tables if they dont already exist, then copy each of the 3 files to their destination tables\n",
    "9. Finally, data check are run to validate that the table row count matches the dataframe row count\n",
    "\n",
    "\n",
    "The pipeline architecture was chose with efficiency and cost in mind. As Bixi generates a lot of data from their infrastructure, it needs to be stored in scalable storage, which S3 allows. Furthermore, leveraging the distributed computing power of Spark allows this pipeline even more scalability as this data will likely increase in volume\n",
    "\n",
    "Given that the trip data is available monthly, the pipeline would also be run monthly.\n",
    "\n",
    "Future use case scenarios:\n",
    "* If the data were to increase by 100x, the S3 storage would allow scalability. However, the Spark cluster might need to be upgraded with to have additional, more powerful nodes. Same goes for the Redshift cluster. \n",
    "* If a dashboard needed to be produced and updated on a daily basis ay 7am, the pipeline should then also be run daily, early in the monring to allow ample time for execution. The trip data would also need to be made available more often (For the purposes of this project, the only source was the Bixi open data platform which is updated monthly, however Bixi would presumable have access to live data). Additionally, to handle the daily loads, this pipeline could be set up as an Airflow DAG which would handle scheduling, and retries upon failure, and backfill.\n",
    "* If 100+ people needed access to the database, while Redshift should allow scalability, the infrastructure could benefit from an added load balancer (e.g. Kubernetes cluster) which would better control concurrent traffic. IAM user management will allow to give specific specific permissions on the database as specific by specific roles. Furthermore, the notebook should be moved to execute on an EMR cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}