{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bixi Telemetry Data Warehouse\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The goal of this project is the build a data warehouse for Bixi, a Montreal-based bike-sharing service. The modeled data will then be used to analyze ride pattern and availability of bikes at every station, enabling Bixi to optimize replenishment activities.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBFS_SOURCE = \"s3a://bixi-gbfs-data\"\n",
    "GBFS_DESTINATION = \"s3a://bixi-gbfs-data-parquet\"\n",
    "\n",
    "STATION_INFO_DATA = \"https://gbfs.velobixi.com/gbfs/en/station_information.json\"\n",
    "TRIP_APRIL_DATA = \"https://sitewebbixi.s3.amazonaws.com/uploads/docs/biximontreal-rentals-2021-04-87aaed.zip\""
   ]
  },
  {
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "The main dataset used will be a json output of the Bixi API, which exposes their [GBFS (General Bikeshare Feed Specification)](https://github.com/NABSA/gbfs) feed. The API was queries every 5 minutes for 3 weeks, using a Python script scheduled by a cron job. The response json was then uploaded to an Amazon S3 bucket."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We start by provisioning the AWS access ID and secret key in the spark config and instantiate a spark session"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.expanduser(\"~/.aws/credentials\"))\n",
    "access_id = config.get(\"default\", \"aws_access_key_id\") \n",
    "access_key = config.get(\"default\", \"aws_secret_access_key\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.0')\n",
    "conf.set(\"fs.s3a.access.key\", access_id)\n",
    "conf.set(\"fs.s3a.secret.key\", access_key)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "source": [
    "Once the Spark session is created, all the `json` files in the S3 bucket are then read into a Spark dataframe. The nested station data is exploded to individual rows and split by column. The unix epoch timestamp in column `last_updated` is converted to a timestamp, then several time parts are expanded into multiple column, which will later be used to partition the parquet output "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(f\"{GBFS_SOURCE}/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = (df\n",
    "    .select(\"last_updated\",explode(\"data.stations\").alias('stations'))\n",
    "    .select(\"last_updated\", \"stations.*\")    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = df_flat \\\n",
    "    .withColumn(\"last_updated_dt\", F.from_unixtime(\"last_updated\")) \\\n",
    "    .withColumn(\"year\", F.year(\"last_updated_dt\")) \\\n",
    "    .withColumn(\"month\", F.month(\"last_updated_dt\")) \\\n",
    "    .withColumn(\"day\", F.dayofmonth(\"last_updated_dt\")) \\\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"last_updated_dt\")) \\\n",
    "    .withColumn(\"hour\", F.hour(\"last_updated_dt\"))"
   ]
  },
  {
   "source": [
    "Lastly, the dataframe is written back to S3 as a `parquet` file, partitioned by year, month, day"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat.write \\\n",
    "    .partitionBy('year','month','day') \\\n",
    "    .parquet(path = f\"{GBFS_DESTINATION}/gbfs.parquet\", mode = \"overwrite\") "
   ]
  },
  {
   "source": [
    "Next, the Bixi API is called again to obtain the latest station metadata"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output = \"data/station_info.json\"\n",
    "with open(json_output, 'w') as f:\n",
    "    json.dump(requests.get(STATION_INFO_DATA).json(),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_station = spark.read.json(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_station_flat = df_station \\\n",
    "    .select(explode(\"data.stations\").alias('stations')) \\\n",
    "    .select(\"stations.lat\"\n",
    "        , \"stations.lon\"\n",
    "        , 'stations.station_id'\n",
    "        , \"stations.name\"\n",
    "    )"
   ]
  },
  {
   "source": [
    "The dataframe is written back to S3 as a parquet file called `stations.parquet`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_station_flat.write.parquet(\n",
    "    path = f\"{GBFS_DESTINATION}/stations.parquet\", \n",
    "    mode = \"overwrite\"\n",
    ")"
   ]
  },
  {
   "source": [
    "The last dataset is trip data from the month april 2021, which is the same timeframe as the GBFS feed, which needs to be extracted from a `zip` file, read into a spark dataframe from a `csv` file, enriched with time based columns, and finally written back to S3 as a `parquet` file, partition by year, month, day, similar to the GBFS feed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}