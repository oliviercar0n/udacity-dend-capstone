{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bixi Telemetry Data Warehouse\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The goal of this project is to build a data warehouse for Bixi, a Montreal-based bike-sharing service. The modeled data will then be used to analyze ride pattern and availability of bikes at every station, enabling Bixi to optimize replenishment activities.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import psycopg2\n",
    "import sql_queries\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBFS_SOURCE = \"s3a://bixi-gbfs-data\"\n",
    "GBFS_DESTINATION = \"s3a://bixi-gbfs-data-parquet\"\n",
    "\n",
    "STATION_INFO_DATA = \"https://gbfs.velobixi.com/gbfs/en/station_information.json\"\n",
    "TRIP_APRIL_DATA = \"https://sitewebbixi.s3.amazonaws.com/uploads/docs/biximontreal-rentals-2021-04-87aaed.zip\"\n",
    "TRIP_APRIL_DATA_FILE = \"OD_2021_04.csv\""
   ]
  },
  {
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "This project will outline the steps to obtain the raw data, transform and clean it, write it back to an S3 bucket and finally injest it into a Redshift datawarehouse. The source data will be collected from GBFS feed data from Bixi that had been previously extracted every 5 minutes for several weeks and stored in an S3 bucket. The latest station metadata and historical trip data will be retrieved from the Bixi REST API directly. The data will be read, cleansed and transfered using a Spark cluster, for efficiency given the large data size. The data warehouse will be hosted on an Amazon Redshift cluster.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "The main dataset used will be a json output of the Bixi API, which exposes their [GBFS (General Bikeshare Feed Specification)](https://github.com/NABSA/gbfs) feed. The API was queries every 5 minutes for 3 weeks, using a Python script scheduled by a cron job. The response json was then uploaded to an Amazon S3 bucket."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Setting up the Spark cluster and reading in the data\n",
    "\n",
    "We start by provisioning the AWS access ID and secret key in the spark config and instantiate a spark session"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.expanduser(\"~/.aws/credentials\"))\n",
    "access_id = config.get(\"default\", \"aws_access_key_id\") \n",
    "access_key = config.get(\"default\", \"aws_secret_access_key\")\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT = config.get(\"DWH\",\"DWH_PORT\")\n",
    "DWH_ENDPOINT = config.get(\"DWH\",\"DWH_ENDPOINT\")\n",
    "DWH_IAM_ROLE_NAME = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.0')\n",
    "conf.set(\"fs.s3a.access.key\", access_id)\n",
    "conf.set(\"fs.s3a.secret.key\", access_key)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "source": [
    "Once the Spark session is created, all the `json` files in the S3 bucket are then read into a Spark dataframe. The nested station data is exploded to individual rows and split by column. The unix epoch timestamp in column `last_updated` is converted to a timestamp, then several time parts are expanded into multiple column, which will later be used to partition the parquet output "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(f\"{GBFS_SOURCE}/20210413022503.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = (df\n",
    "    .select(\"last_updated\",explode(\"data.stations\").alias('stations'))\n",
    "    .select(\"last_updated\", \"stations.*\")    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = df_flat \\\n",
    "    .withColumn(\"last_updated_dt\", F.from_unixtime(\"last_updated\").cast('timestamp')) \\\n",
    "    .withColumn(\"station_id\", df_flat.station_id.cast('long')) \\\n",
    "    .withColumn(\"year\", F.year(\"last_updated_dt\")) \\\n",
    "    .withColumn(\"month\", F.month(\"last_updated_dt\")) \\\n",
    "    .withColumn(\"day\", F.dayofmonth(\"last_updated_dt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = df_flat.select(\n",
    "    [\"station_id\"] + [c for c in df_flat.columns if c not in [\n",
    "        'station_id','eightd_has_available_keys','last_updated'\n",
    "    ]]\n",
    ")"
   ]
  },
  {
   "source": [
    "Next, the Bixi API is called again to obtain the latest station metadata"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output = \"data/station_info.json\"\n",
    "with open(json_output, 'w') as f:\n",
    "    json.dump(requests.get(STATION_INFO_DATA).json(),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_station = spark.read.json(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_station_flat = df_station \\\n",
    "    .select(explode(\"data.stations\").alias('stations')) \\\n",
    "    .select(\"stations.station_id\"\n",
    "        , \"stations.name\"\n",
    "        , 'stations.lat'\n",
    "        , \"stations.lon\"\n",
    "    )"
   ]
  },
  {
   "source": [
    "The last dataset is trip data from the month april 2021, which is the same timeframe as the GBFS feed, which needs to be extracted from a `zip` file, read into a spark dataframe from a `csv` file, enriched with time based columns, and finally written back to S3 as a `parquet` file, partitioned by year, month, day, similar to the GBFS feed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'data/OD_2021_04.csv'"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "csv_path = f\"data/{TRIP_APRIL_DATA_FILE}\"\n",
    "\n",
    "r = requests.get(TRIP_APRIL_DATA)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extract(TRIP_APRIL_DATA_FILE, path = 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips = spark.read.csv(csv_path, header=True)\n",
    "df_trips = df_trips.withColumn(\"trip_id\",F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips = df_trips \\\n",
    "    .withColumn(\"start_date\", F.from_utc_timestamp(\"start_date\",\"EST\")) \\\n",
    "    .withColumn(\"end_date\", F.from_utc_timestamp(\"end_date\",\"EST\")) \\\n",
    "    .withColumn(\"duration_sec\", df_trips.duration_sec.cast('int')) \\\n",
    "    .withColumn(\"start_station_id\", df_trips.emplacement_pk_start.cast('long')) \\\n",
    "    .withColumn(\"end_station_id\", df_trips.emplacement_pk_end.cast('long')) \\\n",
    "    .withColumn(\"year\", F.year(\"start_date\")) \\\n",
    "    .withColumn(\"month\", F.month(\"start_date\")) \\\n",
    "    .withColumn(\"day\", F.dayofmonth(\"start_date\"))\n",
    "\n",
    "df_trips = df_trips.select([\"trip_id\"] + [c for c in df_trips.columns if c not in [\"trip_id\",\"emplacement_pk_start\",\"emplacement_pk_end\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "Since this data comes from the Bixi API and comes from automated telemetry, we can assume that the data is faily clean. Neverthless there are likely scenarios we would like to exclude.\n",
    "\n",
    "Let's first look like there are any trip data with 0 duration, missing station, or where the end datetime is smaller or equal to the start datetime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 errors found\n"
     ]
    }
   ],
   "source": [
    "errors = df_trips.where(\n",
    "    df_trips.end_date <= df_trips.start_date\n",
    ").collect()\n",
    "\n",
    "print(f\"{str(len(errors))} errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 errors found\n"
     ]
    }
   ],
   "source": [
    "zero_duration = df_trips.where(\n",
    "    df_trips.duration_sec == 0\n",
    ").collect()\n",
    "\n",
    "print(f\"{str(len(zero_duration))} errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 errors found\n"
     ]
    }
   ],
   "source": [
    "missing_station = df_trips.where(\n",
    "    df_trips.start_station_id.isNull() | \\\n",
    "    df_trips.end_station_id.isNull()\n",
    ").collect()\n",
    "\n",
    "print(f\"{str(len(missing_station))} errors found\")"
   ]
  },
  {
   "source": [
    "No errors found there. We can also check if we have trips that started and ended at the same station. Since this could be a valid use case for someone going on a ride for fun, we can restrict to a duration of no more than 2 min. Such trips are likely not real trips and should be excluded."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "778 errors found\n"
     ]
    }
   ],
   "source": [
    "dummy_trip = df_trips.where(\n",
    "    (df_trips.start_station_id == df_trips.end_station_id) & \\\n",
    "    (df_trips.duration_sec < 120)\n",
    ")\n",
    "\n",
    "print(f\"{str(len(dummy_trip.collect()))} errors found\")"
   ]
  },
  {
   "source": [
    "There are many of such instances. Let us remove them from the data frame"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_clean = df_trips.join(dummy_trip, on='trip_id', how='left_anti')"
   ]
  },
  {
   "source": [
    "Lastly, the transformed and cleaned dataframe are written back to an S3 bucket as `parquet` and `csv` files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbfs_file_destination = f\"{GBFS_DESTINATION}/gbfs.parquet\"\n",
    "\n",
    "df_flat.write \\\n",
    "    .partitionBy('year','month','day') \\\n",
    "    .parquet(path = gbfs_file_destination, mode = \"overwrite\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_destination = f\"{GBFS_DESTINATION}/stations.csv\"\n",
    "df_station_flat.write.csv(\n",
    "    path = stations_destination, \n",
    "    mode = \"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_destination = f\"{GBFS_DESTINATION}/trips.parquet\"\n",
    "df_trips_clean.write \\\n",
    "    .partitionBy('year','month', 'day') \\\n",
    "    .parquet(\n",
    "        path = trips_destination, \n",
    "        mode = \"overwrite\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    dbname= DWH_DB, \n",
    "    host= DWH_ENDPOINT, \n",
    "    port= DWH_PORT, \n",
    "    user= DWH_DB_USER, \n",
    "    password= DWH_DB_PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query):\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        cur.execute(query)\n",
    "        try:\n",
    "            results = cur.fetchall()\n",
    "        except:\n",
    "            results = None\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        cur.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in sql_queries.create_queries:\n",
    "    run_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_destination = f\"{GBFS_DESTINATION}/trips.parquet\"\n",
    "\n",
    "copy_trips = sql_queries.copy_table_query.format(\n",
    "    'trips',trips_destination.replace('s3a','s3'), access_id, access_key, 'parquet'\n",
    ")\n",
    "run_query(copy_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_destination = f\"{GBFS_DESTINATION}/stations.csv\"\n",
    "copy_stations = sql_queries.copy_table_query.format(\n",
    "    'stations',stations_destination.replace('s3a','s3'), access_id, access_key, 'csv'\n",
    ")\n",
    "run_query(copy_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbfs_file_destination = f\"{GBFS_DESTINATION}/gbfs.parquet\"\n",
    "copy_gbfs= sql_queries.copy_table_query.format(\n",
    "    'gbfs',gbfs_file_destination.replace('s3a','s3'), access_id, access_key, 'parquet'\n",
    ")\n",
    "run_query(copy_gbfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "Next, we will ensure the pipeline ran sucessfully by comparing the row count in the source dataframe with the row count of the resulting table. An error will be thrown for any errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Check for table gbfs\n",
    "df_count = df_flat.count()\n",
    "tbl_count = run_query(sql_queries.check_row_count.format(\"gbfs\"))[0][0]\n",
    "if df_count != tbl_count:\n",
    "    raise ValueError (\"Mismatch of row count between parquet file and table gbfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Check for table stations\n",
    "df_count = df_station_flat.count()\n",
    "tbl_count = run_query(sql_queries.check_row_count.format(\"stations\"))[0][0]\n",
    "if df_count != tbl_count:\n",
    "    raise ValueError (f\"Mismatch of row count between parquet file and table stations. Expected {df_count}, got {tbl_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Check for table trips\n",
    "df_count = df_trips_clean.count()\n",
    "tbl_count = run_query(sql_queries.check_row_count.format(\"trips\"))[0][0]\n",
    "if df_count != tbl_count:\n",
    "    raise ValueError (f\"Mismatch of row count between parquet file and table trips. Expected {df_count}, got {tbl_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "`trips`\n",
    "\n",
    "| Column | Data Type | Description |\n",
    "|:--- |:--- |:---|\n",
    "| trip_id | **integer** | Unique identifier for the trips, added during transformation|\n",
    "| start_date | **timestamp** | Start datetime of the trip|\n",
    "| end_date | **timestamp** | End datetime of the trip|\n",
    "| duration_sec | **integer** | Trip duration in seconds|\n",
    "| is_member | **boolean** | Flag identifying if trip was made by a Bixi member|\n",
    "| start_station_id | **integer** | Station identifier where trip began\n",
    "| end_startion_id | **integer** | Station identifier where trip ended\n",
    "\n",
    "`station`\n",
    "\n",
    "| Column | Data Type | Description |\n",
    "|:--- |:--- |:---|\n",
    "| station_id | **integer** | Unique identifier for the station|\n",
    "| name | **varchar** | Long name of the station|\n",
    "| lat | **decimal** | Latitude of the station location|\n",
    "| lon | **decimal** | Longitude of the station location|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people.\n",
    "\n",
    " Given the nature of the data, this pipeline should be run every day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}